{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier for Spam Detection\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Total Points: 10\n",
    "\n",
    "Complete this notebook and submit it. The notebook needs to be a complete project report with \n",
    "\n",
    "* your implementation,\n",
    "* documentation including a short discussion of how your implementation works and your design choices, and\n",
    "* experimental results (e.g., tables and charts with simulation results) with a short discussion of what they mean. \n",
    "\n",
    "Use the provided notebook cells and insert additional code and markdown cells as needed.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "A spam detection agent gets as its percepts text messages and needs to decide if they are spam or not.\n",
    "Create a [naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) for the \n",
    "[UCI SMS Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) to perform this task.\n",
    "\n",
    "## Create a bag-of-words representation of the text messages [3 Points]\n",
    "\n",
    "The first step is to tokenize the text. Here is an example of how to use the [natural language tool kit (nltk)](https://www.nltk.org/) to create tokens (separate words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text message: \"ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "\"\n",
      "tokens: ['ham', 'Go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'there', 'got', 'amore', 'wat', '...']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# You need to install nltk and then download the tokenizer once.\n",
    "# nltk.download('punkt')\n",
    "\n",
    "file = open(\"smsspamcollection/SMSSpamCollection\", \"r\")\n",
    "\n",
    "sentence = file.readline()\n",
    "print(f\"text message: \\\"{sentence}\\\"\")\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "print(f\"tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with removing frequent words (called [stopwords](https://en.wikipedia.org/wiki/Stop_word)) and very infrequent words so you end up with a reasonable number of words used in the classifier. Maybe you need to remove digits or all non-letter characters. You may also use a stemming algorithm. \n",
    "\n",
    "Convert the tokenized data into a data structure that indicates for each for document what words it contains. The data structure can be a [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix) with 0s and 1s, a pandas dataframe or some sparse matrix structure. Make sure the data structure can be used to split the data into training and test documents (see below).\n",
    "\n",
    "Report the 20 most frequent and the 20 least frequent words in your data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFreq(tokens):\n",
    "    fdist = FreqDist(tokens)\n",
    "    print('Most Common:')\n",
    "    print(fdist.most_common(20))\n",
    "    print('Least Common:')\n",
    "    print(fdist.most_common()[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download 'stopwords' and 'punkt' from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hhe99\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hhe99\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hhe99\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description and code goes here!\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "messages=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    line = file.readline()\n",
    "    if not line:\n",
    "        break\n",
    "    words = nltk.word_tokenize(line)\n",
    "    labels.append(words.pop(0))\n",
    "    # first word is the label\n",
    "    words_clean = []\n",
    "    for word in words:\n",
    "        # Remove punctuations, and non-letter words\n",
    "        if word.isalpha():\n",
    "            # lower case, lemmatize, and stem words\n",
    "            word = lmtzr.lemmatize(word.lower())\n",
    "            word = porter.stem(word)\n",
    "            # remove stop words\n",
    "            if word not in stop:\n",
    "                words_clean.append(porter.stem(word))\n",
    "    messages.append(words_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the tokenized text to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(list(zip(labels, messages)), columns=['Label', 'Message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>[free, entri, wkli, comp, win, fa, cup, final, tkt, may, text, fa, receiv, entri, question, std, txt, rate, c, appli]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>[nah, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>[freemsg, hey, darl, week, word, back, like, fun, still, tb, ok, xxx, std, chg, send, rcv]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4  spam   \n",
       "\n",
       "                                                                                                                 Message  \n",
       "0                                                                                           [ok, lar, joke, wif, u, oni]  \n",
       "1  [free, entri, wkli, comp, win, fa, cup, final, tkt, may, text, fa, receiv, entri, question, std, txt, rate, c, appli]  \n",
       "2                                                                          [u, dun, say, earli, hor, u, c, alreadi, say]  \n",
       "3                                                                            [nah, think, go, usf, life, around, though]  \n",
       "4                             [freemsg, hey, darl, week, word, back, like, fun, still, tb, ok, xxx, std, chg, send, rcv]  "
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5573 entries, 0 to 5572\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Label    5573 non-null   object\n",
      " 1   Message  5573 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 43.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn parameters [3 Points]\n",
    "\n",
    "Use 80% of the data (called training set; randomly chosen) to learn the parameters of the naive Bayes classifier (prior probabilities and likelihoods). Use [Laplacian smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) for the counts.\n",
    "\n",
    "Report the top 20 words (highest conditional probability) for spam and for ham (i.e., not spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index()\n",
    "test = test.reset_index()\n",
    "train.drop('index', axis=1, inplace=True)\n",
    "test.drop('index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the constants needed for the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = [word for message in list(train[train['Label']=='spam']['Message']) for word in message]\n",
    "ham = [word for message in list(train[train['Label']=='ham']['Message']) for word in message]\n",
    "\n",
    "messages = list(train['Message'])\n",
    "# total number of words\n",
    "n_total = len([word for message in messages for word in message])\n",
    "\n",
    "# total number of words in spam\n",
    "n_spam = len(spam)\n",
    "\n",
    "# total number of words in ham\n",
    "n_ham = len(ham)\n",
    "\n",
    "# P(Spam) and P(Ham)\n",
    "p_spam = n_spam / n_total\n",
    "p_ham = n_ham / n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38870"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7674"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31196"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19742732184203757"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8025726781579624"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique words\n",
    "vocabulary = set([word for message in messages for word in message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocabulary = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5268"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of unique words\n",
    "n_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laplace smoothing\n",
    "alpha=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_spam = {}\n",
    "params_ham = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in vocabulary:\n",
    "    # number of occurances of this word in spam messages\n",
    "    n_word_spam = spam.count(word)\n",
    "    # probability of a word given that it's a spam\n",
    "    p_word_spam = (n_word_spam + alpha) / (n_spam + alpha * n_vocabulary)\n",
    "    params_spam[word] = p_word_spam\n",
    "    \n",
    "    n_word_ham = ham.count(word)\n",
    "    p_word_ham = (n_word_ham + alpha) / (n_ham + alpha * n_vocabulary)\n",
    "    params_ham[word] = p_word_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_params_spam = sorted(params_spam.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_params_ham = sorted(params_ham.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTop20(arr):\n",
    "    for count, ele in enumerate(arr):\n",
    "        if count >= 20:\n",
    "            break\n",
    "        print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('call', 0.02287127182815639)\n",
      "('free', 0.012594653067532066)\n",
      "('u', 0.010122083140163808)\n",
      "('txt', 0.009813011899242776)\n",
      "('ur', 0.009040333796940195)\n",
      "('text', 0.008499459125328387)\n",
      "('mobil', 0.008344923504867872)\n",
      "('stop', 0.007263174161644259)\n",
      "('repli', 0.006567763869571936)\n",
      "('claim', 0.006567763869571936)\n",
      "('prize', 0.0058723535774996135)\n",
      "('c', 0.005795085767269356)\n",
      "('thi', 0.005408746716118065)\n",
      "('get', 0.005176943285427291)\n",
      "('servic', 0.005022407664966775)\n",
      "('onli', 0.004945139854736517)\n",
      "('send', 0.0044815329933549685)\n",
      "('tone', 0.004249729562664194)\n",
      "('award', 0.004172461752433936)\n",
      "('cash', 0.004172461752433936)\n"
     ]
    }
   ],
   "source": [
    "getTop20(sorted_params_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('u', 0.02289929793769197)\n",
      "('go', 0.009735629662132515)\n",
      "('get', 0.008117595436594998)\n",
      "('gt', 0.007157744624835454)\n",
      "('lt', 0.007130320315928039)\n",
      "('come', 0.0065544098288723126)\n",
      "('call', 0.006170469504168495)\n",
      "('thi', 0.005813953488372093)\n",
      "('ok', 0.005731680561649847)\n",
      "('like', 0.0056219833260201845)\n",
      "('know', 0.0055945590171127685)\n",
      "('love', 0.005567134708205353)\n",
      "('ur', 0.005567134708205353)\n",
      "('good', 0.005484861781483106)\n",
      "('got', 0.005457437472575691)\n",
      "('wa', 0.005347740236946029)\n",
      "('time', 0.005018648530057042)\n",
      "('day', 0.004908951294427381)\n",
      "('want', 0.004881526985519965)\n",
      "('need', 0.0038394032470381746)\n"
     ]
    }
   ],
   "source": [
    "getTop20(sorted_params_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify a message\n",
    "def classify(message):\n",
    "    # initialize the probability to the prior probability\n",
    "    p_message_spam = p_spam\n",
    "    p_message_ham = p_ham\n",
    "    for word in message:\n",
    "        if word in params_spam:\n",
    "            p_message_spam *= params_spam[word]\n",
    "        if word in params_ham:\n",
    "            p_message_ham *= params_ham[word]\n",
    "    \n",
    "    if p_message_spam > p_message_ham:\n",
    "        return ['spam', p_message_spam, p_message_ham]\n",
    "    else:\n",
    "        return ['ham', p_message_spam, p_message_ham]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the classification performance [4 Points] \n",
    "\n",
    "Classify the remaining 20% of the data (test set) and calculate classification accuracy. Accuracy is defined as the proportion of correctly classified test documents.\n",
    "\n",
    "Inspect a few misclassified text messages and discuss why the classification failed.\n",
    "\n",
    "Discuss how you deal with words in the test data that you have not seen in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Prediction'] = test['Message'].apply(classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Message</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[give, fli, monkey, wot, think, certainli, mind, ani, friend, mine]</td>\n",
       "      <td>[ham, 8.61715992838398e-34, 3.302015933496212e-29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[ye, small, kid, boost, secret, energi]</td>\n",
       "      <td>[ham, 1.6149412439604153e-19, 8.972829299980703e-19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>[dont, think, need, yellow, card, uk, travel, ask, someon, ha, gone, befor, lt, gt, buck]</td>\n",
       "      <td>[ham, 2.444941620396973e-53, 5.592578761184567e-47]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>[sorri, din, lock, keypad]</td>\n",
       "      <td>[ham, 3.6430326127375436e-13, 5.005797007672329e-11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>[hi, hope, ur, day, good, back, walk, tabl, book, half, eight, let, know, ur, come]</td>\n",
       "      <td>[ham, 2.2312537227742784e-48, 1.5254958738966097e-42]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ham</td>\n",
       "      <td>[ok, thk, got, u, wan, come, wat]</td>\n",
       "      <td>[ham, 7.144602368292925e-25, 1.7563342773717436e-17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spam</td>\n",
       "      <td>[last, chanc, claim, ur, worth, discount, ye, offer, mobil, c, sub, remov, txt, x, stop]</td>\n",
       "      <td>[spam, 2.9856528660801005e-42, 6.062354414604601e-52]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>[bank, say, money]</td>\n",
       "      <td>[ham, 7.286065225475087e-13, 8.605203427474813e-10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ham</td>\n",
       "      <td>[horribl, gal, sch, stuff, come, u, got, mc]</td>\n",
       "      <td>[ham, 1.1907670613821544e-26, 2.550913808645503e-21]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ham</td>\n",
       "      <td>[good, afternoon, love, ani, job, prospect, miss, lazi, bleak, hmmm, happi, fill, love]</td>\n",
       "      <td>[ham, 4.93333629476174e-45, 6.468160862420722e-37]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ham</td>\n",
       "      <td>[ãœ, neva, tell, noe, home, da, aft, wat]</td>\n",
       "      <td>[ham, 2.1070525952821006e-32, 7.115832699351284e-24]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ham</td>\n",
       "      <td>[hou, ani, beer]</td>\n",
       "      <td>[ham, 1.730440491050333e-12, 2.234730806996575e-10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ham</td>\n",
       "      <td>[dunno, u, ask]</td>\n",
       "      <td>[ham, 5.965465903357727e-11, 3.3864448506764766e-08]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ham</td>\n",
       "      <td>[ok, thanx, take, care]</td>\n",
       "      <td>[ham, 2.5334023732528116e-15, 1.9954345116754896e-11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ham</td>\n",
       "      <td>[go, write, msg, put, dictionari, mode, cover, screen, hand, press, lt, gt, gentli, remov, ur, hand, interest]</td>\n",
       "      <td>[ham, 9.110270209981203e-65, 1.731435789309035e-55]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ham</td>\n",
       "      <td>[plan, famili, set, stone]</td>\n",
       "      <td>[ham, 2.814891525836458e-17, 2.6704330398072282e-14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ham</td>\n",
       "      <td>[call, check, life, begin, qatar, pl, pray, veri, hard]</td>\n",
       "      <td>[ham, 1.3218095358972157e-32, 5.608944997322387e-29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ham</td>\n",
       "      <td>[sir, wait, mail]</td>\n",
       "      <td>[ham, 2.7322744595531575e-12, 9.8526798246249e-10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ham</td>\n",
       "      <td>[suck, got, plan, yo, valentin, yo, valentin]</td>\n",
       "      <td>[ham, 3.272336962576913e-27, 4.922740641169126e-23]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ham</td>\n",
       "      <td>[also, know, lunch, menu, onli, da, know]</td>\n",
       "      <td>[ham, 4.786961499541084e-25, 3.344906143655016e-20]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  \\\n",
       "0    ham   \n",
       "1    ham   \n",
       "2    ham   \n",
       "3    ham   \n",
       "4    ham   \n",
       "5    ham   \n",
       "6   spam   \n",
       "7    ham   \n",
       "8    ham   \n",
       "9    ham   \n",
       "10   ham   \n",
       "11   ham   \n",
       "12   ham   \n",
       "13   ham   \n",
       "14   ham   \n",
       "15   ham   \n",
       "16   ham   \n",
       "17   ham   \n",
       "18   ham   \n",
       "19   ham   \n",
       "\n",
       "                                                                                                           Message  \\\n",
       "0                                              [give, fli, monkey, wot, think, certainli, mind, ani, friend, mine]   \n",
       "1                                                                          [ye, small, kid, boost, secret, energi]   \n",
       "2                        [dont, think, need, yellow, card, uk, travel, ask, someon, ha, gone, befor, lt, gt, buck]   \n",
       "3                                                                                       [sorri, din, lock, keypad]   \n",
       "4                              [hi, hope, ur, day, good, back, walk, tabl, book, half, eight, let, know, ur, come]   \n",
       "5                                                                                [ok, thk, got, u, wan, come, wat]   \n",
       "6                         [last, chanc, claim, ur, worth, discount, ye, offer, mobil, c, sub, remov, txt, x, stop]   \n",
       "7                                                                                               [bank, say, money]   \n",
       "8                                                                     [horribl, gal, sch, stuff, come, u, got, mc]   \n",
       "9                          [good, afternoon, love, ani, job, prospect, miss, lazi, bleak, hmmm, happi, fill, love]   \n",
       "10                                                                       [ãœ, neva, tell, noe, home, da, aft, wat]   \n",
       "11                                                                                                [hou, ani, beer]   \n",
       "12                                                                                                 [dunno, u, ask]   \n",
       "13                                                                                         [ok, thanx, take, care]   \n",
       "14  [go, write, msg, put, dictionari, mode, cover, screen, hand, press, lt, gt, gentli, remov, ur, hand, interest]   \n",
       "15                                                                                      [plan, famili, set, stone]   \n",
       "16                                                         [call, check, life, begin, qatar, pl, pray, veri, hard]   \n",
       "17                                                                                               [sir, wait, mail]   \n",
       "18                                                                   [suck, got, plan, yo, valentin, yo, valentin]   \n",
       "19                                                                       [also, know, lunch, menu, onli, da, know]   \n",
       "\n",
       "                                               Prediction  \n",
       "0      [ham, 8.61715992838398e-34, 3.302015933496212e-29]  \n",
       "1    [ham, 1.6149412439604153e-19, 8.972829299980703e-19]  \n",
       "2     [ham, 2.444941620396973e-53, 5.592578761184567e-47]  \n",
       "3    [ham, 3.6430326127375436e-13, 5.005797007672329e-11]  \n",
       "4   [ham, 2.2312537227742784e-48, 1.5254958738966097e-42]  \n",
       "5    [ham, 7.144602368292925e-25, 1.7563342773717436e-17]  \n",
       "6   [spam, 2.9856528660801005e-42, 6.062354414604601e-52]  \n",
       "7     [ham, 7.286065225475087e-13, 8.605203427474813e-10]  \n",
       "8    [ham, 1.1907670613821544e-26, 2.550913808645503e-21]  \n",
       "9      [ham, 4.93333629476174e-45, 6.468160862420722e-37]  \n",
       "10   [ham, 2.1070525952821006e-32, 7.115832699351284e-24]  \n",
       "11    [ham, 1.730440491050333e-12, 2.234730806996575e-10]  \n",
       "12   [ham, 5.965465903357727e-11, 3.3864448506764766e-08]  \n",
       "13  [ham, 2.5334023732528116e-15, 1.9954345116754896e-11]  \n",
       "14    [ham, 9.110270209981203e-65, 1.731435789309035e-55]  \n",
       "15   [ham, 2.814891525836458e-17, 2.6704330398072282e-14]  \n",
       "16   [ham, 1.3218095358972157e-32, 5.608944997322387e-29]  \n",
       "17     [ham, 2.7322744595531575e-12, 9.8526798246249e-10]  \n",
       "18    [ham, 3.272336962576913e-27, 4.922740641169126e-23]  \n",
       "19    [ham, 4.786961499541084e-25, 3.344906143655016e-20]  "
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly classified: 1083\n",
      "Incorrectly classified: 32\n",
      "Accuracy 0.9713004484304932\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = len(test)\n",
    "incorrect_index = []\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    if row['Label'] == row['Prediction'][0]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect_index.append(index)\n",
    "\n",
    "print('Correctly classified:', correct)\n",
    "print('Incorrectly classified:', total-correct)\n",
    "print('Accuracy', correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect incorrectly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Message</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ham</td>\n",
       "      <td>[daili, text, favour, thi, time]</td>\n",
       "      <td>[spam, 2.6796062122009853e-16, 2.2024934481914673e-16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>ham</td>\n",
       "      <td>[pa, select]</td>\n",
       "      <td>[spam, 1.697332610665774e-07, 6.760421558806067e-08]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>ham</td>\n",
       "      <td>[call, messag, miss, call]</td>\n",
       "      <td>[spam, 1.9730362874214963e-10, 1.296197799692232e-10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>spam</td>\n",
       "      <td>[hungri, gay, guy, feel, hungri, call, stop, text, call]</td>\n",
       "      <td>[ham, 8.779477295895075e-28, 2.3597092969522187e-27]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>spam</td>\n",
       "      <td>[money, wine, number, wot, next]</td>\n",
       "      <td>[ham, 4.948136471393865e-19, 1.6179777109355818e-16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>ham</td>\n",
       "      <td>[u, get, pic, msg, phone]</td>\n",
       "      <td>[spam, 1.3630273749419212e-13, 1.2977343855050226e-13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>ham</td>\n",
       "      <td>[call, messag, miss, call]</td>\n",
       "      <td>[spam, 1.9730362874214963e-10, 1.296197799692232e-10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>spam</td>\n",
       "      <td>[link, pictur, ha, sent, also, use, http]</td>\n",
       "      <td>[ham, 9.939723523827372e-25, 3.7404444048198545e-24]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>ham</td>\n",
       "      <td>[plea, send, aunti, number]</td>\n",
       "      <td>[spam, 7.428498736682411e-13, 5.724924686930673e-13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>ham</td>\n",
       "      <td>[number, u, live]</td>\n",
       "      <td>[spam, 1.1274730557346101e-08, 9.952001193824748e-09]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>ham</td>\n",
       "      <td>[send, ur, birthdat, month, year, tel, u, ur, life, partner, name, method, calcul, repli, must]</td>\n",
       "      <td>[spam, 1.3319754469100602e-43, 4.493124732170689e-44]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>ham</td>\n",
       "      <td>[network, technic, support, associ]</td>\n",
       "      <td>[spam, 6.2842312569722614e-12, 2.4830342299961953e-12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>ham</td>\n",
       "      <td>[custom, place, call]</td>\n",
       "      <td>[spam, 5.6612726801941415e-09, 1.541964256827637e-09]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>spam</td>\n",
       "      <td>[hear, new, divorc, barbi, come, ken, stuff]</td>\n",
       "      <td>[ham, 3.458258017369779e-19, 2.7077098518881167e-16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>spam</td>\n",
       "      <td>[guess, somebodi, know, secretli, fanci, wan, na, find, give, u, call, landlin]</td>\n",
       "      <td>[ham, 2.0934491335242785e-33, 5.743761770494566e-33]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>spam</td>\n",
       "      <td>[let, send, free, anonym, mask, messag, im, send, thi, messag, see, potenti, abu]</td>\n",
       "      <td>[ham, 3.296271037420546e-29, 1.604126334465041e-28]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>ham</td>\n",
       "      <td>[avail, soir, june]</td>\n",
       "      <td>[spam, 1.092909783821263e-12, 5.462675305991629e-13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>ham</td>\n",
       "      <td>[laid, airtel, line, rest]</td>\n",
       "      <td>[spam, 8.444674577509373e-16, 8.171459730132278e-17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>ham</td>\n",
       "      <td>[handl, victoria, island, traffic, plu, album, due]</td>\n",
       "      <td>[spam, 1.0083472611826289e-24, 7.374834440444953e-25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>ham</td>\n",
       "      <td>[anytim]</td>\n",
       "      <td>[spam, 0.0001372929915452278, 6.603000313936726e-05]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>ham</td>\n",
       "      <td>[alright, new, goal]</td>\n",
       "      <td>[spam, 1.930807284750898e-11, 1.5725883456642568e-11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>ham</td>\n",
       "      <td>[like, new, mobil]</td>\n",
       "      <td>[spam, 6.777133569475651e-09, 1.8664140628804727e-09]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>spam</td>\n",
       "      <td>[wo, believ, true, incr, txt, repli, g, learn, truli, amaz, thing, blow, mind, onli]</td>\n",
       "      <td>[ham, 6.109444208977585e-47, 9.670309946091991e-45]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>spam</td>\n",
       "      <td>[ask, chatlin, inclu, free, min, india, cust, serv, sed, ye, got, mega, bill, dont, giv, shit, bailiff, due, day, want]</td>\n",
       "      <td>[ham, 6.351010387108896e-63, 1.6877653431194066e-61]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>spam</td>\n",
       "      <td>[wo, believ, true, incr, txt, repli, g, learn, truli, amaz, thing, blow, mind, onli]</td>\n",
       "      <td>[ham, 6.109444208977585e-47, 9.670309946091991e-45]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>ham</td>\n",
       "      <td>[channel]</td>\n",
       "      <td>[spam, 3.050955367671729e-05, 2.201000104645575e-05]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>ham</td>\n",
       "      <td>[hmmm, mani, player, select]</td>\n",
       "      <td>[spam, 8.106887594408997e-15, 6.014194361377356e-15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>spam</td>\n",
       "      <td>[uniqu, enough, find, august]</td>\n",
       "      <td>[ham, 5.629783051672916e-16, 1.9066739370308648e-15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>ham</td>\n",
       "      <td>[urgh, coach, hot, smell, chip, fat, thank, especi, duvet, predict, text, word]</td>\n",
       "      <td>[spam, 9.721924021766541e-33, 7.581568660227521e-33]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>spam</td>\n",
       "      <td>[guess, somebodi, know, secretli, fanci, wan, na, find, give, u, call, landlin]</td>\n",
       "      <td>[ham, 2.0934491335242785e-33, 5.743761770494566e-33]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>spam</td>\n",
       "      <td>[hi, babe, chloe, r, u, wa, smash, saturday, night, wa, great, wa, weekend, u, miss, sp, text, stop, stop]</td>\n",
       "      <td>[ham, 6.355047953615029e-54, 1.8507276331238294e-50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>ham</td>\n",
       "      <td>[free, call]</td>\n",
       "      <td>[spam, 5.687007208291823e-05, 6.51897793170585e-06]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label  \\\n",
       "24     ham   \n",
       "87     ham   \n",
       "105    ham   \n",
       "106   spam   \n",
       "109   spam   \n",
       "123    ham   \n",
       "148    ham   \n",
       "205   spam   \n",
       "270    ham   \n",
       "292    ham   \n",
       "301    ham   \n",
       "366    ham   \n",
       "369    ham   \n",
       "372   spam   \n",
       "393   spam   \n",
       "409   spam   \n",
       "418    ham   \n",
       "441    ham   \n",
       "467    ham   \n",
       "526    ham   \n",
       "722    ham   \n",
       "799    ham   \n",
       "815   spam   \n",
       "878   spam   \n",
       "897   spam   \n",
       "918    ham   \n",
       "951    ham   \n",
       "955   spam   \n",
       "959    ham   \n",
       "1025  spam   \n",
       "1058  spam   \n",
       "1099   ham   \n",
       "\n",
       "                                                                                                                      Message  \\\n",
       "24                                                                                           [daili, text, favour, thi, time]   \n",
       "87                                                                                                               [pa, select]   \n",
       "105                                                                                                [call, messag, miss, call]   \n",
       "106                                                                  [hungri, gay, guy, feel, hungri, call, stop, text, call]   \n",
       "109                                                                                          [money, wine, number, wot, next]   \n",
       "123                                                                                                 [u, get, pic, msg, phone]   \n",
       "148                                                                                                [call, messag, miss, call]   \n",
       "205                                                                                 [link, pictur, ha, sent, also, use, http]   \n",
       "270                                                                                               [plea, send, aunti, number]   \n",
       "292                                                                                                         [number, u, live]   \n",
       "301                           [send, ur, birthdat, month, year, tel, u, ur, life, partner, name, method, calcul, repli, must]   \n",
       "366                                                                                       [network, technic, support, associ]   \n",
       "369                                                                                                     [custom, place, call]   \n",
       "372                                                                              [hear, new, divorc, barbi, come, ken, stuff]   \n",
       "393                                           [guess, somebodi, know, secretli, fanci, wan, na, find, give, u, call, landlin]   \n",
       "409                                         [let, send, free, anonym, mask, messag, im, send, thi, messag, see, potenti, abu]   \n",
       "418                                                                                                       [avail, soir, june]   \n",
       "441                                                                                                [laid, airtel, line, rest]   \n",
       "467                                                                       [handl, victoria, island, traffic, plu, album, due]   \n",
       "526                                                                                                                  [anytim]   \n",
       "722                                                                                                      [alright, new, goal]   \n",
       "799                                                                                                        [like, new, mobil]   \n",
       "815                                      [wo, believ, true, incr, txt, repli, g, learn, truli, amaz, thing, blow, mind, onli]   \n",
       "878   [ask, chatlin, inclu, free, min, india, cust, serv, sed, ye, got, mega, bill, dont, giv, shit, bailiff, due, day, want]   \n",
       "897                                      [wo, believ, true, incr, txt, repli, g, learn, truli, amaz, thing, blow, mind, onli]   \n",
       "918                                                                                                                 [channel]   \n",
       "951                                                                                              [hmmm, mani, player, select]   \n",
       "955                                                                                             [uniqu, enough, find, august]   \n",
       "959                                           [urgh, coach, hot, smell, chip, fat, thank, especi, duvet, predict, text, word]   \n",
       "1025                                          [guess, somebodi, know, secretli, fanci, wan, na, find, give, u, call, landlin]   \n",
       "1058               [hi, babe, chloe, r, u, wa, smash, saturday, night, wa, great, wa, weekend, u, miss, sp, text, stop, stop]   \n",
       "1099                                                                                                             [free, call]   \n",
       "\n",
       "                                                  Prediction  \n",
       "24    [spam, 2.6796062122009853e-16, 2.2024934481914673e-16]  \n",
       "87      [spam, 1.697332610665774e-07, 6.760421558806067e-08]  \n",
       "105    [spam, 1.9730362874214963e-10, 1.296197799692232e-10]  \n",
       "106     [ham, 8.779477295895075e-28, 2.3597092969522187e-27]  \n",
       "109     [ham, 4.948136471393865e-19, 1.6179777109355818e-16]  \n",
       "123   [spam, 1.3630273749419212e-13, 1.2977343855050226e-13]  \n",
       "148    [spam, 1.9730362874214963e-10, 1.296197799692232e-10]  \n",
       "205     [ham, 9.939723523827372e-25, 3.7404444048198545e-24]  \n",
       "270     [spam, 7.428498736682411e-13, 5.724924686930673e-13]  \n",
       "292    [spam, 1.1274730557346101e-08, 9.952001193824748e-09]  \n",
       "301    [spam, 1.3319754469100602e-43, 4.493124732170689e-44]  \n",
       "366   [spam, 6.2842312569722614e-12, 2.4830342299961953e-12]  \n",
       "369    [spam, 5.6612726801941415e-09, 1.541964256827637e-09]  \n",
       "372     [ham, 3.458258017369779e-19, 2.7077098518881167e-16]  \n",
       "393     [ham, 2.0934491335242785e-33, 5.743761770494566e-33]  \n",
       "409      [ham, 3.296271037420546e-29, 1.604126334465041e-28]  \n",
       "418     [spam, 1.092909783821263e-12, 5.462675305991629e-13]  \n",
       "441     [spam, 8.444674577509373e-16, 8.171459730132278e-17]  \n",
       "467    [spam, 1.0083472611826289e-24, 7.374834440444953e-25]  \n",
       "526     [spam, 0.0001372929915452278, 6.603000313936726e-05]  \n",
       "722    [spam, 1.930807284750898e-11, 1.5725883456642568e-11]  \n",
       "799    [spam, 6.777133569475651e-09, 1.8664140628804727e-09]  \n",
       "815      [ham, 6.109444208977585e-47, 9.670309946091991e-45]  \n",
       "878     [ham, 6.351010387108896e-63, 1.6877653431194066e-61]  \n",
       "897      [ham, 6.109444208977585e-47, 9.670309946091991e-45]  \n",
       "918     [spam, 3.050955367671729e-05, 2.201000104645575e-05]  \n",
       "951     [spam, 8.106887594408997e-15, 6.014194361377356e-15]  \n",
       "955     [ham, 5.629783051672916e-16, 1.9066739370308648e-15]  \n",
       "959     [spam, 9.721924021766541e-33, 7.581568660227521e-33]  \n",
       "1025    [ham, 2.0934491335242785e-33, 5.743761770494566e-33]  \n",
       "1058    [ham, 6.355047953615029e-54, 1.8507276331238294e-50]  \n",
       "1099     [spam, 5.687007208291823e-05, 6.51897793170585e-06]  "
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "test.iloc[incorrect_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some messages are incorrectly classified but by a very small difference. For example, messages 270, 366, 722 are all down to the last decimal place. These messages contain many neutral words that have similar probabilities in both spam and ham. Another potential reason that these messages are incorrectly classified is because the classifier does not take into account words that it has not seen before. Third potential reason is that the classifier does not take into account the order of the words and therefore cannot get a sense of context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that the classifier has not seen are ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus task [+1 Point]\n",
    "\n",
    "Describe how you could improve the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would use TF-IDF to imporve the classifier. TF-IDF stands for term frequency-inverse document frequency. It effectively diminishes the impact of words that occur a lot. For example, if the word \"like\" appears in almost every message, the classifier would treat it as less influencial than words that do not appear as frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each word:<br>\n",
    "    TF = number of occurences of a word given the message is spam<br>\n",
    "    IDF = log(total number of messages / total number of message containing the word)<br>\n",
    "    p(word|spam) = (TF * IDF) / (sum of all words' TF * IDF) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
